{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiWlFnxPUh71",
        "outputId": "6e8a13a2-8725-4ea4-ce1a-07eac0d3a9d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded from: /content/drive/MyDrive/FewShot\n",
            "Epoch 1/5 - Loss: 0.7705, Accuracy: 0.7000\n",
            "Epoch 2/5 - Loss: 0.8141, Accuracy: 0.8000\n",
            "Epoch 3/5 - Loss: 0.6710, Accuracy: 0.7000\n",
            "Epoch 4/5 - Loss: 0.6681, Accuracy: 0.7000\n",
            "Epoch 5/5 - Loss: 0.6509, Accuracy: 0.7000\n",
            "Class Names: ['Cyclone', 'Wildfire']\n",
            "Uploaded image is classified as: Cyclone\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "\n",
        "# Load images from the specified folder\n",
        "def load_images_from_folder(folder_path, img_size=(28, 28)):\n",
        "    \"\"\"Loads images from the given folder and labels them based on folder names.\"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = os.listdir(folder_path)  # List folder names as class names\n",
        "    for class_id, class_name in enumerate(class_names):\n",
        "        class_folder = os.path.join(folder_path, class_name)\n",
        "        if os.path.isdir(class_folder):  # Process only subdirectories (classes)\n",
        "            for filename in os.listdir(class_folder):\n",
        "                img_path = os.path.join(class_folder, filename)\n",
        "                img = load_img(img_path, target_size=img_size, color_mode=\"grayscale\")\n",
        "                img_array = img_to_array(img) / 255.0  # Normalize to [0, 1]\n",
        "                images.append(img_array)\n",
        "                labels.append(class_id)\n",
        "    return np.array(images), np.array(labels), class_names\n",
        "\n",
        "# Define the Prototypical Network\n",
        "class ProtoNet(Model):\n",
        "    def __init__(self):\n",
        "        super(ProtoNet, self).__init__()\n",
        "        self.conv1 = Conv2D(64, (3, 3), padding='same')\n",
        "        self.bn1 = BatchNormalization()\n",
        "        self.pool1 = MaxPooling2D((2, 2))\n",
        "        self.flatten = Flatten()\n",
        "        self.fc = Dense(64, activation='relu')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = ReLU()(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.flatten(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# Compute prototypes\n",
        "def compute_prototypes(model, x_support, y_support, num_classes):\n",
        "    \"\"\"Compute class prototypes.\"\"\"\n",
        "    embeddings = model(x_support)\n",
        "    prototypes = []\n",
        "    for c in range(num_classes):\n",
        "        prototypes.append(tf.reduce_mean(embeddings[y_support == c], axis=0))\n",
        "    return tf.stack(prototypes)\n",
        "\n",
        "# Classify query images\n",
        "def classify_query_images(model, x_query, prototypes):\n",
        "    \"\"\"Classify query images based on distance from nearest prototype.\"\"\"\n",
        "    query_embeddings = model(x_query)\n",
        "    distances = tf.norm(tf.expand_dims(query_embeddings, axis=1) - prototypes, axis=2)\n",
        "    return tf.argmin(distances, axis=1)\n",
        "\n",
        "# Function to upload and preprocess a new image\n",
        "def upload_and_preprocess_image(image_path, target_size=(28, 28)):\n",
        "    \"\"\"Loads and preprocesses the uploaded image.\"\"\"\n",
        "    img = load_img(image_path, target_size=target_size, color_mode=\"grayscale\")\n",
        "    img_array = img_to_array(img) / 255.0  # Normalize to [0, 1]\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    return img_array\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify the dataset path directly in the code\n",
        "    dataset_path = '/content/drive/MyDrive/FewShot'  # Replace with your folder path\n",
        "    print(f\"Dataset loaded from: {dataset_path}\")\n",
        "\n",
        "    # Define parameters\n",
        "    img_size = (28, 28)\n",
        "    shots_per_class = 20\n",
        "    query_per_class = 10\n",
        "\n",
        "    # Load dataset\n",
        "    x_data, y_data, class_names = load_images_from_folder(dataset_path, img_size)\n",
        "    num_classes = len(class_names)\n",
        "\n",
        "    # Split dataset into support and query sets (simulate few-shot)\n",
        "    x_support, x_query, y_support, y_query = train_test_split(\n",
        "        x_data, y_data, test_size=query_per_class / len(x_data), random_state=42\n",
        "    )\n",
        "\n",
        "    # Instantiate the model\n",
        "    model = ProtoNet()\n",
        "\n",
        "    # Train the model\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    epochs = 5\n",
        "    for epoch in range(epochs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            support_embeddings = model(x_support)\n",
        "            query_embeddings = model(x_query)\n",
        "\n",
        "            prototypes = compute_prototypes(model, x_support, y_support, num_classes)\n",
        "\n",
        "            distances = tf.norm(tf.expand_dims(query_embeddings, axis=1) - prototypes, axis=2)\n",
        "            predicted = tf.argmin(distances, axis=1)\n",
        "\n",
        "            loss = tf.keras.losses.sparse_categorical_crossentropy(y_query, distances)\n",
        "            acc = tf.reduce_mean(tf.cast(tf.equal(predicted, y_query), tf.float32))\n",
        "\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} - Loss: {tf.reduce_mean(loss):.4f}, Accuracy: {acc:.4f}\")\n",
        "\n",
        "    # Test the model\n",
        "    prototypes = compute_prototypes(model, x_support, y_support, num_classes)\n",
        "    predictions = classify_query_images(model, x_query, prototypes)\n",
        "    print(\"Class Names:\", class_names)\n",
        "\n",
        "    # Now, classify an uploaded image\n",
        "    image_path = '/content/drive/MyDrive/FewShot/Cyclone/29.jpg'  # Replace with the path to your image\n",
        "    uploaded_image = upload_and_preprocess_image(image_path, target_size=img_size)\n",
        "\n",
        "    # Compute prototypes based on the support set\n",
        "    prototypes = compute_prototypes(model, x_support, y_support, num_classes)\n",
        "\n",
        "    # Classify the uploaded image\n",
        "    image_embedding = model(uploaded_image)\n",
        "    distances = tf.norm(tf.expand_dims(image_embedding, axis=1) - prototypes, axis=2)\n",
        "    predicted_class = tf.argmin(distances, axis=1).numpy()[0]\n",
        "\n",
        "    print(f\"Uploaded image is classified as: {class_names[predicted_class]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify the dataset path directly in the code\n",
        "    dataset_path = '/content/drive/MyDrive/FewShot'  # Replace with your folder path\n",
        "    print(f\"Dataset loaded from: {dataset_path}\")\n",
        "\n",
        "    # Define parameters\n",
        "    img_size = (28, 28)\n",
        "    shots_per_class = 20\n",
        "    query_per_class = 10\n",
        "\n",
        "    # Load dataset\n",
        "    x_data, y_data, class_names = load_images_from_folder(dataset_path, img_size)\n",
        "    num_classes = len(class_names)\n",
        "\n",
        "    # Split dataset into support and query sets (simulate few-shot)\n",
        "    x_support, x_query, y_support, y_query = train_test_split(\n",
        "        x_data, y_data, test_size=query_per_class / len(x_data), random_state=42\n",
        "    )\n",
        "\n",
        "    # Instantiate the model\n",
        "    model = ProtoNet()\n",
        "\n",
        "    # Train the model\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    epochs = 3\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            support_embeddings = model(x_support)\n",
        "            query_embeddings = model(x_query)\n",
        "\n",
        "            prototypes = compute_prototypes(model, x_support, y_support, num_classes)\n",
        "\n",
        "            distances = tf.norm(tf.expand_dims(query_embeddings, axis=1) - prototypes, axis=2)\n",
        "            predicted = tf.argmin(distances, axis=1)\n",
        "\n",
        "            loss = tf.keras.losses.sparse_categorical_crossentropy(y_query, distances)\n",
        "            acc = tf.reduce_mean(tf.cast(tf.equal(predicted, y_query), tf.float32))\n",
        "\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} - Loss: {tf.reduce_mean(loss):.4f}, Accuracy: {acc:.4f}\")\n",
        "\n",
        "    # Test the model\n",
        "    prototypes = compute_prototypes(model, x_support, y_support, num_classes)\n",
        "    predictions = classify_query_images(model, x_query, prototypes)\n",
        "\n",
        "    print(\"Class Names:\", class_names)\n",
        "\n",
        "    # Now, classify an uploaded image\n",
        "    image_path = '/content/drive/MyDrive/FewShot/Wildfire/24.jpg'  # Replace with the path to your image\n",
        "    uploaded_image = upload_and_preprocess_image(image_path, target_size=img_size)\n",
        "\n",
        "    # Compute prototypes based on the support set\n",
        "    prototypes = compute_prototypes(model, x_support, y_support, num_classes)\n",
        "\n",
        "    # Classify the uploaded image\n",
        "    image_embedding = model(uploaded_image)\n",
        "    distances = tf.norm(tf.expand_dims(image_embedding, axis=1) - prototypes, axis=2)\n",
        "    predicted_class = tf.argmin(distances, axis=1).numpy()[0]\n",
        "\n",
        "    print(f\"Uploaded image is classified as: {class_names[predicted_class]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gECnSA4Cbb3X",
        "outputId": "10f54b92-52ed-46ea-883b-b5562d7a4aa6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded from: /content/drive/MyDrive/FewShot\n",
            "Epoch 1/3 - Loss: 0.7573, Accuracy: 0.8000\n",
            "Epoch 2/3 - Loss: 0.8613, Accuracy: 0.8000\n",
            "Epoch 3/3 - Loss: 0.6831, Accuracy: 0.7000\n",
            "Class Names: ['Cyclone', 'Wildfire']\n",
            "Uploaded image is classified as: Wildfire\n"
          ]
        }
      ]
    }
  ]
}